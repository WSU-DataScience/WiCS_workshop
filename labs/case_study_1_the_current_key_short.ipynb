{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study - The Current\n",
    "\n",
    "* The Current is an alternative radio station\n",
    "* We will pull information about the play list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Exercise - Go the the following page and inspect the following </font>\n",
    "\n",
    "* Song title\n",
    "* Artist\n",
    "* Play time\n",
    "* Day, date, period (am/pm)\n",
    "\n",
    "http://www.thecurrent.org/playlist/2014-01-01/01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_url = 'http://www.thecurrent.org/playlist/2014-01-01/01'\n",
    "s = requests.Session()\n",
    "r = s.get(example_url)\n",
    "\n",
    "soup = bs4.BeautifulSoup(r.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull off the period of the day (am/pm)\n",
    "\n",
    "Pull out the \"am\"/\"pm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'am'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period = soup('span', class_=\"hour-header open\")[0].next.split('to')[0].rstrip()[-2:]\n",
    "period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day of the Week\n",
    "\n",
    "* Pull out the day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wednesday'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_of_week = soup('a', class_=\"start-picker\")[0].next.split(',')[0]\n",
    "day_of_week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title of each song\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Holy Roller',\n",
       " 'Kingdom of Rust',\n",
       " 'Black Dog',\n",
       " 'Turn It Around',\n",
       " 'Flavor of the Month',\n",
       " 'Potential Wife',\n",
       " '24 Hours',\n",
       " \"Who's Gonna Shoe Your Pretty Little Feet?\",\n",
       " 'Marigold',\n",
       " 'High Road',\n",
       " 'The Vampyre Of Time and Memory',\n",
       " 'Valerie Plame',\n",
       " 'Morning Song',\n",
       " '(You Will) Set The World On Fire',\n",
       " 'Sixteen Saltines',\n",
       " 'Wave of Mutilation']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = [tag.next.strip() for tag in soup.findAll('h5', class_ = \"title\")]\n",
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Name of the Artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thao and The Get Down Stay Down',\n",
       " 'Doves',\n",
       " 'Frankie Lee',\n",
       " 'Lucius',\n",
       " 'The Posies',\n",
       " 'Strange Names',\n",
       " 'Sky Ferreira',\n",
       " 'Billie Joe and Norah',\n",
       " 'J. Roddy Walston and The Business',\n",
       " 'Cults',\n",
       " 'Queens of the Stone Age',\n",
       " 'The Decemberists',\n",
       " 'The Avett Brothers',\n",
       " 'David Bowie',\n",
       " 'Jack White',\n",
       " 'Pixies']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists = [tag.next.strip() for tag in  soup.findAll('h5',class_='artist')]\n",
    "artists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Song Start Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1:59',\n",
       " '1:54',\n",
       " '1:51',\n",
       " '1:46',\n",
       " '1:44',\n",
       " '1:38',\n",
       " '1:34',\n",
       " '1:31',\n",
       " '1:27',\n",
       " '1:23',\n",
       " '1:19',\n",
       " '1:13',\n",
       " '1:09',\n",
       " '1:05',\n",
       " '1:03',\n",
       " '1:01']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_times = [tag.time.next.strip() for tag in  soup('div', class_=\"two columns songTime\")]\n",
    "start_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_period(soup):\n",
    "    search = soup('span', class_=\"hour-header open\")\n",
    "    if len(search) > 0:\n",
    "        return search[0].next.split('to')[0].rstrip()[-2:]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_day(soup):\n",
    "    search = soup('a', class_=\"start-picker\")\n",
    "    if len(search) > 0:\n",
    "        return search[0].next.split(',')[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_song_info(url):\n",
    "    print(\"Starting {0} urls\".format(url))\n",
    "    date = url.split('/')[-2]\n",
    "    s = requests.Session()\n",
    "    r = s.get(url)\n",
    "    soup = bs4.BeautifulSoup(r.content, 'lxml')\n",
    "    period = get_period(soup)\n",
    "    day_of_week = get_day(soup)\n",
    "    soup = bs4.BeautifulSoup(r.content)\n",
    "    titles = [t.next.strip() for t in soup.findAll('h5', class_=\"title\")]\n",
    "    artists = [a.next.strip() for a in soup.findAll('h5',class_='artist')]\n",
    "    times = [d.time.next.strip() for d in soup('div', class_=\"two columns songTime\")]\n",
    "    song_info = [(day_of_week, date, time, period, title, artist) \n",
    "             for time, title, artist in zip(times, titles, artists)]\n",
    "    return song_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting http://www.thecurrent.org/playlist/2014-01-01/01 urls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tiverson/.pyenv/versions/anaconda3-5.0.0/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/tiverson/.pyenv/versions/anaconda3-5.0.0/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Wednesday',\n",
       "  '2014-01-01',\n",
       "  '1:59',\n",
       "  'am',\n",
       "  'Holy Roller',\n",
       "  'Thao and The Get Down Stay Down'),\n",
       " ('Wednesday', '2014-01-01', '1:54', 'am', 'Kingdom of Rust', 'Doves'),\n",
       " ('Wednesday', '2014-01-01', '1:51', 'am', 'Black Dog', 'Frankie Lee'),\n",
       " ('Wednesday', '2014-01-01', '1:46', 'am', 'Turn It Around', 'Lucius'),\n",
       " ('Wednesday',\n",
       "  '2014-01-01',\n",
       "  '1:44',\n",
       "  'am',\n",
       "  'Flavor of the Month',\n",
       "  'The Posies'),\n",
       " ('Wednesday', '2014-01-01', '1:38', 'am', 'Potential Wife', 'Strange Names'),\n",
       " ('Wednesday', '2014-01-01', '1:34', 'am', '24 Hours', 'Sky Ferreira'),\n",
       " ('Wednesday',\n",
       "  '2014-01-01',\n",
       "  '1:31',\n",
       "  'am',\n",
       "  \"Who's Gonna Shoe Your Pretty Little Feet?\",\n",
       "  'Billie Joe and Norah'),\n",
       " ('Wednesday',\n",
       "  '2014-01-01',\n",
       "  '1:27',\n",
       "  'am',\n",
       "  'Marigold',\n",
       "  'J. Roddy Walston and The Business'),\n",
       " ('Wednesday', '2014-01-01', '1:23', 'am', 'High Road', 'Cults'),\n",
       " ('Wednesday',\n",
       "  '2014-01-01',\n",
       "  '1:19',\n",
       "  'am',\n",
       "  'The Vampyre Of Time and Memory',\n",
       "  'Queens of the Stone Age'),\n",
       " ('Wednesday',\n",
       "  '2014-01-01',\n",
       "  '1:13',\n",
       "  'am',\n",
       "  'Valerie Plame',\n",
       "  'The Decemberists'),\n",
       " ('Wednesday',\n",
       "  '2014-01-01',\n",
       "  '1:09',\n",
       "  'am',\n",
       "  'Morning Song',\n",
       "  'The Avett Brothers'),\n",
       " ('Wednesday',\n",
       "  '2014-01-01',\n",
       "  '1:05',\n",
       "  'am',\n",
       "  '(You Will) Set The World On Fire',\n",
       "  'David Bowie'),\n",
       " ('Wednesday', '2014-01-01', '1:03', 'am', 'Sixteen Saltines', 'Jack White'),\n",
       " ('Wednesday', '2014-01-01', '1:01', 'am', 'Wave of Mutilation', 'Pixies')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_song_info(example_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting a years worth of data\n",
    "\n",
    "## Step 1 - Identify the url pattern\n",
    "\n",
    "The Current uses urls of the following pattern\n",
    "\n",
    "    'http://www.thecurrent.org/playlist/2017-05-04/10'\n",
    "\n",
    "or \n",
    "\n",
    "    'http://www.thecurrent.org/playlist/year-month-day/hour'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: How might you generate all combinations for a given year?\n",
    "\n",
    "**Answer:** Python has a tool for that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "numdays = 365\n",
    "base = datetime.datetime.today()\n",
    "dts = [base - datetime.timedelta(hours = h) for h in range(0, numdays*24)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_address(dt):\n",
    "    fmt = 'http://www.thecurrent.org/playlist/%Y-%m-%d/%H'\n",
    "    return dt.strftime(fmt)\n",
    "\n",
    "def test_output_address():\n",
    "    date = datetime.datetime(2000,1,1,1)\n",
    "    assert output_address(date) == 'http://www.thecurrent.org/playlist/2000-01-01/01'\n",
    "test_output_address()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.thecurrent.org/playlist/2017-05-14/16',\n",
       " 'http://www.thecurrent.org/playlist/2017-05-14/15',\n",
       " 'http://www.thecurrent.org/playlist/2017-05-14/14',\n",
       " 'http://www.thecurrent.org/playlist/2017-05-14/13',\n",
       " 'http://www.thecurrent.org/playlist/2017-05-14/12',\n",
       " 'http://www.thecurrent.org/playlist/2017-05-14/11',\n",
       " 'http://www.thecurrent.org/playlist/2017-05-14/10',\n",
       " 'http://www.thecurrent.org/playlist/2017-05-14/09',\n",
       " 'http://www.thecurrent.org/playlist/2017-05-14/08',\n",
       " 'http://www.thecurrent.org/playlist/2017-05-14/07']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [output_address(d) for d in dts]\n",
    "urls[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import pipe\n",
    "from toolz.curried import take\n",
    "from more_itertools import side_effect, consume\n",
    "\n",
    "with open('the_current_last_year.csv', 'w') as outfile:\n",
    "    header = \"Weekday,Date,Time,Period,Song_Title,Artist\"\n",
    "    print(header, file=outfile)\n",
    "    count = 0\n",
    "    for url in urls:\n",
    "        for song_info in get_song_info(url):\n",
    "            count += 1\n",
    "            if count % 5 == 0:\n",
    "                print(\"Processed {0} songs\".format(count))\n",
    "            join_info = \",\".join(song_info)\n",
    "            print(join_info, file=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
